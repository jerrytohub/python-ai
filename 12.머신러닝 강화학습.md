# 강화학습
## 강화학습 5요소
* 환경(env) : 변화하는 가상의 공간
* 에이전트(agent) : 환경을 탐색하면서 시행착오를 반복하는 개체(학습자)
* 상태(state) : 에이전트에게 주어진 세분화된 환경
* 동작(action) : 에이전트가 상태에 대응하기 위해서 선택하는 동작
* 보상(reward) : 에어전트가 동작을 한 결과(보상 또는 벌)

## Frozen Lake
* Fronze lake는 ChatGPT를 만든 OpenAI의 GYM 프로젝트 중 하나입니다.
* 에이전트가 꽁꽁 언 호수를 지나서 목표지점에 도착하는 미션입니다.
* 강화학습은 에이전트를 환경에 적응시키는 것입니다.
  * 시행착오를 통해 성공과 실패를 반복하면서 행동을 개선합니다.
* 알파고, ChatGPT가 강화학습으로 개발되었습니다.
![image](https://github.com/jerrytohub/python-ai/assets/127598703/c34561e9-d21b-41f7-bc4d-f85caa7ad0eb)

## Q-러닝 
* Q-테이블을 이용하여 에이전트의 동작을 개선하는 것입니다.
* Q-테이블은 상태별 보상을 행, 선택 가능한 동작을 열로 표시합니다.
* 에이전트는 동작의 결과가 반영된 Q-테이블을 참조하여 최선의 동작을 선택합니다.
* A는 동작, S는 상태를 나타냅니다.
* S0에서 A0을 선택하면 0점이 되고 A2를 선택하면 5점이 됩니다.
  * Frozen Lake에서 A는 에이전트가 움직이는 방향입니다.

| Q | A0 | A1 | A2 | A3 |
|:---:|:---:|:---:|:---:|:---:|
| S0 | 0 | 0 | 5 | 10 |
| S1 | 5 | 10 | 10 | 0 |
| S2 | 0 | 15 | 0 | 0 |
| S3 | 0 | 20 | 0 | 0 |


* 에이전트는 시행을 반복하면서 Q-테이블을 업데이트합니다.
* 처음에는 경험치가 없기 때문에 랜덤하게 움직입니다.
* 그 이후에는 시행착오를 거듭하면서 Q-테이블을 더 많이 참조하면서 동작을 개선합니다.
* 학습반영률, 보상할인율, 무작위동작률 개념을 이해합니다.
  * 학습반영률 α(알파)
    * 학습반영률은 Q-테이블 정보를 얼마나 반영할 것인지 정합니다.
    * 학습반영률이 높으면 최신 Q-테이블 정보를 적극적으로 반영합니다.
    * 학습반영률이 낮으면 최신 Q-테이블 정보를 소극적으로 반영합니다.
  * 보상할인율 γ(감마)
    * 보상할인율은 에이전트가 현재의 보상에 집중할 것인지, 미래의 보상에 집중할 것인지 정합니다.
    * 보상할인율이 높으면 현재의 보상보다는 미래의 보상에 집중합니다.
    * 보상할인율이 높으면 미래의 보상보다는 현재의 보상에 집중합니다.
  * 무작위동작률 ϵ(입실론)
    * 입실론은 0~1사이의 값입니다. 
    * 얼마나 랜덤하게 움직일 것인지 정합니다.
    * 값이 높으면 새로운 길을 찾습니다.
      * exploration : 새로운 길을 찾는 것
    * 값이 낮으면 Q-테이블을 이용해서 길을 찾습니다.
      * exploitation : Q-테이블을 이용하는 것 
    * 처음에는 높은 값에서 시작하다가 나중에 감소합니다.
    * 무작위로 동작하지 않는다면 같던 길로만 이동합니다.      

# 실제 환경에서 하기
## VSCODE 설정
* VSCODE에서 설정합니다.
* Ctrl + Shift + P 단축키로 명령 팔레트를 엽니다.
* select default profile을 검색하고 ```Terminal : Select Default Profile```을 선택합니다.
![image](https://github.com/jerrytohub/python-ai/assets/127598703/72e26a0a-3f44-45a9-bd0a-cf704b9f1c8a)
 
