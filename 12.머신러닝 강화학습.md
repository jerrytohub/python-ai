# 강화학습
## 강화학습 5요소
* 환경(env) : 변화하는 가상의 공간
* 에이전트(agent) : 환경을 탐색하면서 시행착오를 반복하는 개체(학습자)
* 상태(state) : 에이전트에게 주어진 세분화된 환경
* 동작(action) : 에이전트가 상태에 대응하기 위해서 선택하는 동작
* 보상(reward) : 에어전트가 동작을 한 결과(보상 또는 벌)

## Frozen Lake
* Fronze lake는 ChatGPT를 만든 OpenAI의 GYM 프로젝트 중 하나입니다.
* 에이전트가 꽁꽁 언 호수를 지나서 목표지점에 도착하는 미션입니다.
* 강화학습은 에이전트를 환경에 적응시키는 것입니다.
  * 시행착오를 통해 성공과 실패를 반복하면서 행동을 개선합니다.
* 알파고, ChatGPT가 강화학습으로 개발되었습니다.
![image](https://github.com/jerrytohub/python-ai/assets/127598703/c34561e9-d21b-41f7-bc4d-f85caa7ad0eb)

## Q-러닝 
* Q-테이블을 이용하여 에이전트의 동작을 개선하는 것입니다.
* Q-테이블은 상태별 보상을 행, 선택 가능한 동작을 열로 표시합니다.
* 에이전트는 동작의 결과가 반영된 Q-테이블을 참조하여 최선의 동작을 선택합니다.
* A는 동작, S는 상태를 나타냅니다.
* S0에서 A0을 선택하면 0점이 되고 A2를 선택하면 5점이 됩니다.
  * Frozen Lake에서 A는 에이전트가 움직이는 방향입니다.

| Q | A0 | A1 | A2 | A3 |
|:---:|:---:|:---:|:---:|:---:|
| S0 | 0 | 0 | 5 | 10 |
| S1 | 5 | 10 | 10 | 0 |
| S2 | 0 | 15 | 0 | 0 |
| S3 | 0 | 20 | 0 | 0 |


* 에이전트는 시행을 반복하면서 Q-테이블을 업데이트합니다.
* 처음에는 경험치가 없기 때문에 랜덤하게 움직입니다.
* 그 이후에는 시행착오를 거듭하면서 Q-테이블을 더 많이 참조하면서 동작을 개선합니다.
* 학습반영률, 보상할인율, 무작위동작률 개념을 이해합니다.
  * 학습반영률 α(알파)
    * 학습반영률은 Q-테이블 정보를 얼마나 반영할 것인지 정합니다.
    * 학습반영률이 높으면 최신 Q-테이블 정보를 적극적으로 반영합니다.
    * 학습반영률이 낮으면 최신 Q-테이블 정보를 소극적으로 반영합니다.
  * 보상할인율 γ(감마)
    * 보상할인율은 에이전트가 현재의 보상에 집중할 것인지, 미래의 보상에 집중할 것인지 정합니다.
    * 보상할인율이 높으면 현재의 보상보다는 미래의 보상에 집중합니다.
    * 보상할인율이 높으면 미래의 보상보다는 현재의 보상에 집중합니다.
  * 무작위동작률 ϵ(입실론)
    * 입실론은 0~1사이의 값입니다. 
    * 얼마나 랜덤하게 움직일 것인지 정합니다.
    * 값이 높으면 새로운 길을 찾습니다.
      * exploration : 새로운 길을 찾는 것
    * 값이 낮으면 Q-테이블을 이용해서 길을 찾습니다.
      * exploitation : Q-테이블을 이용하는 것 
    * 처음에는 높은 값에서 시작하다가 나중에 감소합니다.
    * 무작위로 동작하지 않는다면 같던 길로만 이동합니다.      

# 실제 환경에서 하기
## VSCODE 설정
* VSCODE에서 설정합니다.
* Ctrl + Shift + P 단축키로 명령 팔레트를 엽니다.
* select default profile을 검색하고 ```Terminal : Select Default Profile```을 선택합니다.
![image](https://github.com/jerrytohub/python-ai/assets/127598703/72e26a0a-3f44-45a9-bd0a-cf704b9f1c8a)

## 코딩하기
* gymnasium와 pygame을 설치해서 실제 컴퓨터에서 작동시켜 보세요.
* gymnasium와 pygame을 설치합니다.
  * ```pip install gymnasium```
  * ```pip install pygame```
```python
import gymnasium as gym
```
* pygame을 사용해서 프로그램을 작동시킵니다.
```python
import gymnasium as gym
from time import sleep

# 환경을 만듭니다.
# map_name='8x8'이면 64칸이 만들어집니다.
env = gym.make('FrozenLake-v1', map_name='4x4', is_slippery = False, render_mode='human')

# 왼쪽 위부터 순서대로 0으로 해서 인덱스가 정해집니다.
# gymnasium에서는 아래와 같이 state를 정합니다.
state = env.reset()[0]
action = env.action_space.sample()
# gymnasium에서는 5개 값을 반환합니다.
new_state, reward, done, info, _= env.step(action)

sleep(10)
```






































```python
import gymnasium as gym
import numpy as np
import matplotlib.pyplot as plt

env = gym.make('FrozenLake-v1', map_name='4x4', is_slippery = False, render_mode='human')
q_table = np.zeros((env.observation_space.n, env.action_space.n))
episodes = 100
learning_rate = 0.8
gamma = 0.9
epsilon = 0.9
epsilon_decay_rate = 0.0001

rewards = np.zeros(episodes)
for i in range(episodes):    
    state = env.reset()[0]   
    done = False   
   
    while(not done):   
        if np.random.uniform(0, 1) < epsilon:
            action = env.action_space.sample()       
        else:
            action = np.argmax(q_table[state,:])            
        new_state, reward, done, info, _ = env.step(action) 
        q_table[state,action] = q_table[state,action] + learning_rate * ( reward + gamma * np.max(q_table[new_state,:]) - q_table[state,action])
        state = new_state  
   
    epsilon = max(epsilon - epsilon_decay_rate, 0) 
    if epsilon == 0 :
        learning_rate = 0.0001  
    if reward == 1:
        rewards[i] = 1         

sum_rewards = np.zeros(episodes)
for i in range(episodes):
    sum_rewards[i] = np.sum(rewards[max(0, i-100):(i+1)])
plt.plot(sum_rewards)
plt.savefig('frozen_lake.png')
```





 
